Description

This project explores the implicit bias of gradient-based optimization methods in underdetermined regression problems. When multiple global minima exist, gradient descent and its variants converge to solutions that are closest to the initialization in Euclidean distance. This bias leads to the minimal norm solutions and characterize the role of initialization in determining the final solution.

Key Features

Theoretical Foundations: Implementation of proofs characterizing the limit iterate x_infinity and its minimal distance properties.

Advanced Optimization: Support for gradient descent, momentum, Nesterov acceleration, and stochastic gradient descent (SGD).
Loss Functions: Includes least squares and Huber loss for regression
Visuals: Shows the iteration process with hyperplanes and iteration versus component norm

Singular Value Decomposition (SVD) for subspace analysis
Quadratic programming for minimal distance validation

Who Is This For?
Researchers in optimization and machine learning.
Graduate students exploring implicit bias in over-parameterized models.
Practitioners interested in understanding the behavior of gradient-based methods.







