Description

This project explores the implicit bias of gradient-based optimization methods in underdetermined regression problems. When multiple global minima exist, gradient descent and its variants converge to solutions that are closest to the initialization in Euclidean distance. Through rigorous theoretical analysis and numerical simulations, we demonstrate how this bias leads to minimal norm solutions and characterize the role of initialization in determining the final solution.
